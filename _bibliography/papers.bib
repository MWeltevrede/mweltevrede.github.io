@inproceedings{weltevrede_training_2024,
	selected={true},
	title = {Training on more {Reachable} {Tasks} for {Generalisation} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2410.03565},
	abstract = {In multi-task reinforcement learning, agents train on a fixed set of tasks and have to generalise to new ones. Recent work has shown that increased exploration improves this generalisation, but it remains unclear why exactly that is. In this paper, we introduce the concept of reachability in multi-task reinforcement learning and show that an initial exploration phase increases the number of reachable tasks the agent is trained on. This, and not the increased exploration, is responsible for the improved generalisation, even to unreachable tasks. Inspired by this, we propose a novel method Explore-Go that implements such an exploration phase at the beginning of each episode. Explore-Go only modifies the way experience is collected and can be used with most existing on-policy or off-policy reinforcement learning algorithms. We demonstrate the effectiveness of our method when combined with some popular algorithms and show an increase in generalisation performance across several environments.},
	urldate = {2024-10-11},
	publisher = {arXiv},
	author = {Weltevrede, Max and Horsch, Caroline and Spaan, Matthijs T. J. and Böhmer, Wendelin},
	month = {Oct},
	year = {2024},
	arxiv = {2410.03565},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	booktitle = {Preprint. Under Review},
}

@inproceedings{weltevrede_explore-go_2024,
	selected={true},
	title = {Explore-{Go}: {Leveraging} {Exploration} for {Generalisation} in {Deep} {Reinforcement} {Learning}},
	shorttitle = {Explore-{Go}},
	url = {http://arxiv.org/abs/2406.08069},
	abstract = {One of the remaining challenges in reinforcement learning is to develop agents that can generalise to novel scenarios they might encounter once deployed. This challenge is often framed in a multi-task setting where agents train on a fixed set of tasks and have to generalise to new tasks. Recent work has shown that in this setting increased exploration during training can be leveraged to increase the generalisation performance of the agent. This makes sense when the states encountered during testing can actually be explored during training. In this paper, we provide intuition why exploration can also benefit generalisation to states that cannot be explicitly encountered during training. Additionally, we propose a novel method Explore-Go that exploits this intuition by increasing the number of states on which the agent trains. Explore-Go effectively increases the starting state distribution of the agent and as a result can be used in conjunction with most existing on-policy or off-policy reinforcement learning algorithms. We show empirically that our method can increase generalisation performance in an illustrative environment and on the Procgen benchmark.},
	urldate = {2024-10-11},
	publisher = {arXiv},
	author = {Weltevrede, Max and Kaubek, Felix and Spaan, Matthijs T. J. and Böhmer, Wendelin},
	month = {Sep},
	year = {2024},
	arxiv = {2406.08069},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	booktitle = {Seventeenth European Workshop on Reinforcement Learning},
}

@inproceedings{weltevrede_role_2023,
	selected={true},
	title = {The {Role} of {Diverse} {Replay} for {Generalisation} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2306.05727},
	abstract = {In reinforcement learning (RL), key components of many algorithms are the exploration strategy and replay buffer. These strategies regulate what environment data is collected and trained on and have been extensively studied in the RL literature. In this paper, we investigate the impact of these components in the context of generalisation in multi-task RL. We investigate the hypothesis that collecting and training on more diverse data from the training environments will improve zero-shot generalisation to new tasks. We motivate mathematically and show empirically that generalisation to tasks that are "reachable'' during training is improved by increasing the diversity of transitions in the replay buffer. Furthermore, we show empirically that this same strategy also shows improvement for generalisation to similar but "unreachable'' tasks which could be due to improved generalisation of the learned latent representations.},
	urldate = {2024-10-11},
	publisher = {arXiv},
	author = {Weltevrede, Max and Spaan, Matthijs T. J. and Böhmer, Wendelin},
	month = {Aug},
	year = {2023},
	arxiv = {2306.05727},
	keywords = {Computer Science - Machine Learning},
	booktitle = {Sixteenth European Workshop on Reinforcement Learning},
}
