@inproceedings{weltevrede_training_2024,
	selected={true},
	title = {Exploration Implies Data Augmentation: Reachability and Generalisation in Contextual MDPs},
	url = {http://arxiv.org/abs/2410.03565},
	abstract = {In the zero-shot policy transfer (ZSPT) setting for contextual Markov decision processes (MDP), agents train on a fixed set of contexts and must generalise to new ones. Recent work has argued and demonstrated that increased exploration can improve this generalisation, by training on more states in the training contexts. In this paper, we demonstrate that training on more states can indeed improve generalisation, but can come at a cost of reducing the accuracy of the learned value function which should not benefit generalisation. We introduce reachability in the ZSPT setting to define which states/contexts require generalisation and explain why exploration can improve it. We hypothesise and demonstrate that using exploration to increase the agent's coverage while also increasing the accuracy improves generalisation even more. Inspired by this, we propose a method Explore-Go that implements an exploration phase at the beginning of each episode, which can be combined with existing on- and off-policy RL algorithms and significantly improves generalisation even in partially observable MDPs. We demonstrate the effectiveness of Explore-Go when combined with several popular algorithms and show an increase in generalisation performance across several environments. With this, we hope to provide practitioners with a simple modification that can improve the generalisation of their agents.},
	urldate = {2024-10-11},
	publisher = {arXiv},
	author = {Weltevrede, Max and Horsch, Caroline and Spaan, Matthijs T. J. and Böhmer, Wendelin},
	month = {Feb},
	year = {2025},
	arxiv = {2410.03565},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	booktitle = {Preprint. Under Review},
}

@inproceedings{weltevrede_explore-go_2024,
	selected={true},
	title = {Explore-{Go}: {Leveraging} {Exploration} for {Generalisation} in {Deep} {Reinforcement} {Learning}},
	shorttitle = {Explore-{Go}},
	url = {http://arxiv.org/abs/2406.08069},
	abstract = {One of the remaining challenges in reinforcement learning is to develop agents that can generalise to novel scenarios they might encounter once deployed. This challenge is often framed in a multi-task setting where agents train on a fixed set of tasks and have to generalise to new tasks. Recent work has shown that in this setting increased exploration during training can be leveraged to increase the generalisation performance of the agent. This makes sense when the states encountered during testing can actually be explored during training. In this paper, we provide intuition why exploration can also benefit generalisation to states that cannot be explicitly encountered during training. Additionally, we propose a novel method Explore-Go that exploits this intuition by increasing the number of states on which the agent trains. Explore-Go effectively increases the starting state distribution of the agent and as a result can be used in conjunction with most existing on-policy or off-policy reinforcement learning algorithms. We show empirically that our method can increase generalisation performance in an illustrative environment and on the Procgen benchmark.},
	urldate = {2024-10-11},
	publisher = {arXiv},
	author = {Weltevrede, Max and Kaubek, Felix and Spaan, Matthijs T. J. and Böhmer, Wendelin},
	month = {Sep},
	year = {2024},
	arxiv = {2406.08069},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	booktitle = {Seventeenth European Workshop on Reinforcement Learning},
}

@inproceedings{weltevrede_role_2023,
	selected={true},
	title = {The {Role} of {Diverse} {Replay} for {Generalisation} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2306.05727},
	abstract = {In reinforcement learning (RL), key components of many algorithms are the exploration strategy and replay buffer. These strategies regulate what environment data is collected and trained on and have been extensively studied in the RL literature. In this paper, we investigate the impact of these components in the context of generalisation in multi-task RL. We investigate the hypothesis that collecting and training on more diverse data from the training environments will improve zero-shot generalisation to new tasks. We motivate mathematically and show empirically that generalisation to tasks that are "reachable'' during training is improved by increasing the diversity of transitions in the replay buffer. Furthermore, we show empirically that this same strategy also shows improvement for generalisation to similar but "unreachable'' tasks which could be due to improved generalisation of the learned latent representations.},
	urldate = {2024-10-11},
	publisher = {arXiv},
	author = {Weltevrede, Max and Spaan, Matthijs T. J. and Böhmer, Wendelin},
	month = {Aug},
	year = {2023},
	arxiv = {2306.05727},
	keywords = {Computer Science - Machine Learning},
	booktitle = {Sixteenth European Workshop on Reinforcement Learning},
}
